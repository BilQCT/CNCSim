{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector files:\n",
    "cnc_n_1_files = [(f\"cnc_{n}_1.jld\",\"/cnc_n_1\") for n in range(1,4)]\n",
    "stab_n_files = [(f\"stab_{n}.jld\",\"/stab_n\") for n in range(1,4)]\n",
    "\n",
    "# decomposition files\n",
    "cnc_decomp_n_files = [(f\"cnc_decomposition_{n}.jld\",\"/x_value\") for n in range(1,4)]\n",
    "stab_decomp_n_files = [(f\"stab_decomposition_{n}.jld\",\"/x_value\") for n in range(1,5)]\n",
    "\n",
    "# all relevant data files:\n",
    "data_file_names = cnc_n_1_files+stab_n_files+cnc_decomp_n_files+stab_decomp_n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find current path\n",
    "current = os.getcwd()\n",
    "data_path = os.path.join(current, \"data\")\n",
    "data_files = []\n",
    "# Loop through files in data_path\n",
    "for filename in data_file_names:\n",
    "    file_path = os.path.join(data_path, filename[0])\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        # 'f' is an open HDF5 file object, you can inspect its keys, datasets, etc.\n",
    "        # For example, to list all datasets:\n",
    "        filename_prefix = filename[0][:-4]\n",
    "        data = f[filename[1]][:]\n",
    "        if len(data.dtype) > 1:\n",
    "            data = data[\"num\"]\n",
    "        np.save(\"./data/\"+filename_prefix,np.transpose(data))\n",
    "        #data_files.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some manual data retrieval:\n",
    "with h5py.File(\"./large_data/stab_4.jld\", 'r') as f:\n",
    "        # 'f' is an open HDF5 file object, you can inspect its keys, datasets, etc.\n",
    "        # For example, to list all datasets:\n",
    "        data = np.transpose(f['/stab_n'][:][\"num\"])\n",
    "        np.save(\"./large_data/stab_4\",data)\n",
    "        #data_files.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some manual data retrieval:\n",
    "with h5py.File(\"./large_data/cnc_vectors_4_1.jld\", 'r') as f:\n",
    "        # 'f' is an open HDF5 file object, you can inspect its keys, datasets, etc.\n",
    "        # For example, to list all datasets:\n",
    "        data = np.transpose(f['/cnc_vectors_4_1'][:][\"num\"])\n",
    "        np.save(\"./large_data/cnc_4_1\",data)\n",
    "        #data_files.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some manual data retrieval:\n",
    "cnc_support_4 = None\n",
    "cnc_decomposition_4 = None\n",
    "with h5py.File(\"./data/cnc_decomposition_4_1.jld\", 'r') as f:\n",
    "        # 'f' is an open HDF5 file object, you can inspect its keys, datasets, etc.\n",
    "        # For example, to list all datasets:\n",
    "        #data = f['/cnc_decomposition_4_1']\n",
    "        key = list(f.keys())[1]\n",
    "\n",
    "        # random sample of 80k cnc 4-1 vectors\n",
    "        cnc_samples_4 = f[(f[key][()])[0]][:]\n",
    "        # python indexing:\n",
    "        cnc_samples_4 = [int(x-1) for x in cnc_samples_4]\n",
    "        # optimal decomposition using random sample:\n",
    "        cnc_decomposition_4 = f[(f[key][()])[1]][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.70710678, 0.70710678, 0.        , 0.70710678,\n",
       "       0.5       , 0.5       , 0.        , 0.70710678, 0.5       ,\n",
       "       0.5       , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def t_state(n):\n",
    "    T = np.array([1.0,1/np.sqrt(2),1/np.sqrt(2),0.0])\n",
    "    T_state = T\n",
    "    for i in range(1,n):\n",
    "        T_state = np.kron(T_state,T)\n",
    "    return T_state\n",
    "\n",
    "t_state(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving simulation keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find current path\n",
    "current = os.getcwd()\n",
    "data_path = os.path.join(current, \"data\")\n",
    "data_files = dict()\n",
    "\n",
    "for n in range(1,5):\n",
    "    # generate filenames:\n",
    "    if n == 5:\n",
    "        vec_filename = \"./large_data/cnc_4_1.npy\"\n",
    "    else:\n",
    "        vec_filename = os.path.join(data_path,f\"cnc_{n}_1.npy\")\n",
    "    supp_filename = os.path.join(data_path,f\"cnc_decomposition_{n}.npy\")\n",
    "\n",
    "    # load data\n",
    "    vec_data = np.load(vec_filename)\n",
    "    supp_data = np.load(supp_filename)\n",
    "\n",
    "    # threshold value:\n",
    "    theshold = 10**(-(16))\n",
    "    nonzero_support = [i for i in range(len(supp_data)) if abs(supp_data[i]) > theshold]\n",
    "    data_files[n] = (supp_data[nonzero_support],vec_data[:,nonzero_support])\n",
    "    #data_files.append((supp_data[nonzero_support],vec_data[:,nonzero_support]))\n",
    "    #print(sum(supp_data[nonzero_support]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 4-qubit data:\n",
    "cnc_4_1 = np.load(\"./large_data/cnc_4_1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only non-negligible supports:\n",
    "threshold = 10**(-16)\n",
    "nonzero_support = [i for i in range(len(cnc_decomposition_4)) if abs(cnc_decomposition_4[i]) > threshold]\n",
    "\n",
    "# extract optimal subset:\n",
    "cnc_support_4 = [cnc_samples_4[i] for i in nonzero_support]\n",
    "cnc_optimal_4_1 = cnc_4_1[:,cnc_support_4]\n",
    "cnc_optimal_decomp_4 = cnc_decomposition_4[nonzero_support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 4-qubit data to dictionary:\n",
    "data_files[4] = (cnc_optimal_decomp_4,cnc_optimal_4_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"./keys/all_cnc_keys_4.h5\", \"w\") as f:\n",
    "    for n in range(1,5):\n",
    "        reshaped_vector = np.transpose(data_files[n][0]).reshape(len(data_files[n][0]),1)\n",
    "        concatenated_array = np.concatenate((np.transpose(reshaped_vector),data_files[n][1]),axis = 0)\n",
    "        f.create_dataset(f\"n={n}\", data=concatenated_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find current path\n",
    "current = os.getcwd()\n",
    "data_path = os.path.join(current, \"data\")\n",
    "stab_data_files = dict()\n",
    "\n",
    "for n in range(1,5):\n",
    "    # generate filenames:\n",
    "    if n == 4:\n",
    "        vec_filename = \"./large_data/stab_4.npy\"\n",
    "    else:\n",
    "        vec_filename = os.path.join(data_path,f\"stab_{n}.npy\")\n",
    "    supp_filename = os.path.join(data_path,f\"stab_decomposition_{n}.npy\")\n",
    "\n",
    "    # load data\n",
    "    vec_data = np.load(vec_filename)\n",
    "    supp_data = np.load(supp_filename)\n",
    "\n",
    "    # threshold value:\n",
    "    theshold = 10**(-(16))\n",
    "    nonzero_support = [i for i in range(len(supp_data)) if abs(supp_data[i]) > theshold]\n",
    "    stab_data_files[n] = (supp_data[nonzero_support],vec_data[:,nonzero_support])\n",
    "    #data_files.append((supp_data[nonzero_support],vec_data[:,nonzero_support]))\n",
    "    #print(sum(supp_data[nonzero_support]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"./keys/all_stab_keys_4.h5\", \"w\") as f:\n",
    "    for n in range(1,5):\n",
    "        reshaped_vector = np.transpose(stab_data_files[n][0]).reshape(len(stab_data_files[n][0]),1)\n",
    "        concatenated_array = np.concatenate((np.transpose(reshaped_vector),stab_data_files[n][1]),axis = 0)\n",
    "        f.create_dataset(f\"n={n}\", data=concatenated_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20710678,  0.70710678,  0.5       ],\n",
       "       [ 1.        ,  1.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ],\n",
       "       [-1.        ,  0.        ,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to load data\n",
    "with h5py.File(\"./keys/all_stab_keys_4.h5\", \"r\") as f:\n",
    "    n_data_set = f[\"n=1\"][:]\n",
    "n_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
